{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCKitpZTpISn"
   },
   "source": [
    "# Making necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3D1My5RHncNS"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "cWbZwPp6VBMi",
    "outputId": "62a2e69a-869b-4cb1-83f3-5860e803e2a2"
   },
   "outputs": [],
   "source": [
    "# NLP library imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpT3m7C2o_Qc"
   },
   "source": [
    "# Applying the transformation we've seen to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Premier s\\u00e9jour \\u00e0 center parc pour no...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Petite semaine en famille</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>sejour tr\\u00e8s agreable comme d habitude a c...</td>\n",
       "      <td>fr</td>\n",
       "      <td>center parcs la promesse des daims</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Cottage tr\\u00e8s bien, propre et chaud d\\u00e...</td>\n",
       "      <td>fr</td>\n",
       "      <td>A am\\u00e9liorer</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>S\\u00e9jour tr\\u00e8s reposant \\u00e0 noter to...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Reposant</td>\n",
       "      <td>février 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>2 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Arriv\\u00e9s pour trois nuits et deux jours en...</td>\n",
       "      <td>fr</td>\n",
       "      <td>tres belle decouverte</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       hotel_name  published_date  rating  \\\n",
       "0  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "1  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "2  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "3  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "4  Center Parcs Le Bois aux Daims  2 février 2019       5   \n",
       "\n",
       "                                              review review_language  \\\n",
       "0  Premier s\\u00e9jour \\u00e0 center parc pour no...              fr   \n",
       "1  sejour tr\\u00e8s agreable comme d habitude a c...              fr   \n",
       "2  Cottage tr\\u00e8s bien, propre et chaud d\\u00e...              fr   \n",
       "3  S\\u00e9jour tr\\u00e8s reposant \\u00e0 noter to...              fr   \n",
       "4  Arriv\\u00e9s pour trois nuits et deux jours en...              fr   \n",
       "\n",
       "                                title      trip_date  \n",
       "0           Petite semaine en famille   janvier 2019  \n",
       "1  center parcs la promesse des daims   janvier 2019  \n",
       "2                    A am\\u00e9liorer   janvier 2019  \n",
       "3                            Reposant   février 2019  \n",
       "4               tres belle decouverte   janvier 2019  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_json('don4_frr_spds_420.json')\n",
    "df1 = df1.drop([\"hotel_adress\",\"hotel_type\",\"image_url\",\"locid\",\"pid\",\"price_range\",\"review_id\",\"reviewer_id\",\"trip_type\"], axis = 1)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Premier séjour à center parc pour nous et nous...\n",
       "1       sejour très agreable comme d habitude a center...\n",
       "2       Cottage très bien, propre et chaud dès l'arriv...\n",
       "3       Séjour très reposant à noter tout de même troi...\n",
       "4       Arrivés pour trois nuits et deux jours en sema...\n",
       "5       Center parc est un endroit très relaxant et en...\n",
       "6       agréablement surpris super séjour bien reposer...\n",
       "7       Piscine bondée de monde !! Et pourtant hors pé...\n",
       "8       Le personnel du service client n'a pas été à l...\n",
       "9       Un vrai paradis pour déconnecter du travail, d...\n",
       "10      10 jours de détente en famille, que ce soit au...\n",
       "11      très beau cadre en harmonie avec la naturedéco...\n",
       "12      Séjour très agréable, très bon accueil, cottag...\n",
       "13      le parc est calme , la nature est partout  et ...\n",
       "14      Très bonnes installations. Seul bémol quota de...\n",
       "15      Nous sommes venus en famille avec une amie et ...\n",
       "16      Nous avons passé un très bon séjour! L 'enviro...\n",
       "17      Le cottage VIP 6 personnes était juste super!!...\n",
       "18      Toujours aussi bien j'y vais toujours avec pla...\n",
       "19      Premier séjour à Center Parcs en famille avec ...\n",
       "20      Cottage très bien agencé pour une famille nomb...\n",
       "21      Bel endroit, très calme au niveau des cottages...\n",
       "22      Bon séjour passé en famille le côté nature éta...\n",
       "23      Nous avons passé une super semaine en famille....\n",
       "24      Très dépaysant , surtout en hiver.Très adapté ...\n",
       "25      émerveillement pour les enfants: l'endroit ide...\n",
       "26      Un séjour magique en famille. Nous y sommes al...\n",
       "27      c'est bon de pouvoir se baigner dans une eau&q...\n",
       "28      Un geste commercial serait apprécié compte ten...\n",
       "29      Je recommande center parc à toutes les personn...\n",
       "                              ...                        \n",
       "1964    Cottage très agréable,  parcs très calme et sé...\n",
       "1965    Le concept est reposant, les daims vous renden...\n",
       "1966    Bonnes infrastructures neuves et de qualité ma...\n",
       "1967    Notre troisième séjour à Center parcs est iden...\n",
       "1968    C est la troisième fois que nous venons passer...\n",
       "1969    Passer une journée dans le Center Parcs avis m...\n",
       "1970    Nous sommes partis une semaine en cottage prem...\n",
       "1971    Domaine magnifique dommage que le services dan...\n",
       "1972    Ce séjour à Center Parcs était une grande prem...\n",
       "1973    4 jours passés, très agréable le cottage très ...\n",
       "1974    Nous avons voulu tester ce parc car moins loin...\n",
       "1975    Mes 2 miss de 10 ans ont été éblouies et se so...\n",
       "1976    Le parc, la piscine et le cottage sont très ag...\n",
       "1977    emplacement ok,ménage: moutons de poussière so...\n",
       "1978    Nous avons passés un agréable séjour à Center ...\n",
       "1979    Mon enfant est tombé sur le nez en montant les...\n",
       "1980    Le parc est magnifique, les daims sont bien pr...\n",
       "1981    We très sympa a Center Parcs ressour\\u00e7ant ...\n",
       "1982    VACANCES sympa la cadre et les daims beaux spe...\n",
       "1983    Tres dé\\u00e7u par le bois aux daimsC'est le 2...\n",
       "1984    Ménage fait trop vite et donc mal, puisque pou...\n",
       "1985    Une formule agréable pour des vacances actives...\n",
       "1986    Nous étions partis entre amis avec deux enfant...\n",
       "1987    Personnel très agréable. Centre adapté aux jeu...\n",
       "1988    Nous avons passé un très bon séjour. Le domain...\n",
       "1989    Cottage mal entretenu. Activités très onéreuse...\n",
       "1990    Très bon séjours... De très bonnes activités, ...\n",
       "1991    Très joli parc au milieu d'une forêt. Beaucoup...\n",
       "1992    Je me permet de noter seulement les restaurant...\n",
       "1993    Nous venons de passer 10 jours au domaine, 1 w...\n",
       "Name: review, Length: 1994, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.review = df1.review.apply(lambda x:x.replace(r\"\\u00e8\",\"è\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00e9', 'é'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r\"\\u00ea\",\"ê\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r\"\\u00eb\",\"ë\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r\"\\u00fb\",\"û\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r\"\\u00f9\",\"ù\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00e0', 'à'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00e2', 'â'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00f4', 'ô'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00ee', 'î'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u00ef', 'ï'))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\u2019', \"'\"))\n",
    "df1.review = df1.review.apply(lambda x:x.replace(r'\\'', \"'\"))\n",
    "\n",
    "df1.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               Petite semaine en famille\n",
       "1                      center parcs la promesse des daims\n",
       "2                                             A améliorer\n",
       "3                                                Reposant\n",
       "4                                   tres belle decouverte\n",
       "5                                             Déconnexion\n",
       "6                                         séjour agréable\n",
       "7                                                 Piscine\n",
       "8                                      Bof bof le service\n",
       "9                                           SUPER  SEJOUR\n",
       "10                                        Vraiment cool !\n",
       "11                 relaxation et cocooning au rendez-vous\n",
       "12                                   Confort et bien-être\n",
       "13                                conforme à nos attentes\n",
       "14              Très agréable en famille sauf le week-end\n",
       "15        Séjour en mid-week du 21 janvier au 25 janvier.\n",
       "16                                     Bon séjour sauf...\n",
       "17                               Un bon moment de détente\n",
       "18            Une bulle de décompression en plein hiver !\n",
       "19                              Super moment en famille!!\n",
       "20                                               Relaxant\n",
       "21          Très satisfaits de notre WE, belle découverte\n",
       "22                             Séjour agréable en famille\n",
       "23                                              Excellent\n",
       "24                    Un bon moment de détente en famille\n",
       "25                                         que du bonheur\n",
       "26                           Un séjour à faire en janvier\n",
       "27                                                détente\n",
       "28                                              Déception\n",
       "29      Superbe séjour, je recommande une évasion au S...\n",
       "                              ...                        \n",
       "1964                                        Super sejours\n",
       "1965                                            Reposant \n",
       "1966                                    Bilan mitigé.... \n",
       "1967                    La bulle: le paradis des enfants \n",
       "1968                                          Parc super \n",
       "1969                                      Aqua mundo top \n",
       "1970                           Vacances avec bébé au top!\n",
       "1971                                          Restaurant \n",
       "1972                Un séjour en famille en pleine nature\n",
       "1973                                     Excellent séjour\n",
       "1974                                     Manque de nature\n",
       "1975                                               génial\n",
       "1976                                          Avis mitigé\n",
       "1977                                              bof!!!!\n",
       "1978                                 SEJOUR AVEC DES AMIS\n",
       "1979                                        Enfant blessé\n",
       "1980             Magnifique parc mais quelques faiblesses\n",
       "1981                                             We sympa\n",
       "1982                                                Sympa\n",
       "1983                                           Dé\\u00e7ue\n",
       "1984                                             propreté\n",
       "1985                                     Séjour Août 2017\n",
       "1986       Un très bon séjour mais quelques quoiques !!!!\n",
       "1987                                    on y retournera!!\n",
       "1988           Très bon séjour bon déconnecter en famille\n",
       "1989             de tous les center parcs, c'est le pire.\n",
       "1990                               séjours Bois aux Daims\n",
       "1991                                            Mitigé...\n",
       "1992                        Ne pas aller aux restaurants \n",
       "1993                       séjour bien , mais pas parfait\n",
       "Name: title, Length: 1994, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.title = df1.title.apply(lambda x:x.replace(r\"\\u00e8\",\"è\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00e9', 'é'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r\"\\u00ea\",\"ê\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r\"\\u00eb\",\"ë\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r\"\\u00f9\",\"ù\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00ee', 'î'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00ef', 'ï'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r\"\\u00fb\",\"û\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00e0', 'à'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00e2', 'â'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u00f4', 'ô'))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\u2019', \"'\"))\n",
    "df1.title = df1.title.apply(lambda x:x.replace(r'\\'', \"'\"))\n",
    "\n",
    "df1.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r\"\\u00e8\",\"è\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00e9', 'é'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r\"\\u00ea\",\"ê\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r\"\\u00eb\",\"ë\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r\"\\u00f9\",\"ù\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00ee', 'î'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00ef', 'ï'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r\"\\u00fb\",\"û\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00e0', 'à'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00e2', 'â'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u00f4', 'ô'))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\u2019', \"'\"))\n",
    "df1.trip_date = df1.trip_date.apply(lambda x:x.replace(r'\\'', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Premier séjour à center parc pour nous et nous...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Petite semaine en famille</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>sejour très agreable comme d habitude a center...</td>\n",
       "      <td>fr</td>\n",
       "      <td>center parcs la promesse des daims</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Cottage très bien, propre et chaud dès l'arriv...</td>\n",
       "      <td>fr</td>\n",
       "      <td>A améliorer</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Séjour très reposant à noter tout de même troi...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Reposant</td>\n",
       "      <td>février 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>2 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Arrivés pour trois nuits et deux jours en sema...</td>\n",
       "      <td>fr</td>\n",
       "      <td>tres belle decouverte</td>\n",
       "      <td>janvier 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       hotel_name  published_date  rating  \\\n",
       "0  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "1  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "2  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "3  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "4  Center Parcs Le Bois aux Daims  2 février 2019       5   \n",
       "\n",
       "                                              review review_language  \\\n",
       "0  Premier séjour à center parc pour nous et nous...              fr   \n",
       "1  sejour très agreable comme d habitude a center...              fr   \n",
       "2  Cottage très bien, propre et chaud dès l'arriv...              fr   \n",
       "3  Séjour très reposant à noter tout de même troi...              fr   \n",
       "4  Arrivés pour trois nuits et deux jours en sema...              fr   \n",
       "\n",
       "                                title      trip_date  \n",
       "0           Petite semaine en famille   janvier 2019  \n",
       "1  center parcs la promesse des daims   janvier 2019  \n",
       "2                         A améliorer   janvier 2019  \n",
       "3                            Reposant   février 2019  \n",
       "4               tres belle decouverte   janvier 2019  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemming/Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "V-Lu49mLnke7",
    "outputId": "d5ced2c8-3d38-4309-d0ce-c9bd43d95931"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>14 août 2017</td>\n",
       "      <td>2</td>\n",
       "      <td>Cottage mal entretenu. Activités très onéreuse...</td>\n",
       "      <td>fr</td>\n",
       "      <td>de tous les center parcs, c'est le pire.</td>\n",
       "      <td>août 2017</td>\n",
       "      <td>[mal, entretenu, activites, onereuses, mauvais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>13 août 2017</td>\n",
       "      <td>4</td>\n",
       "      <td>Très bon séjours... De très bonnes activités, ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>séjours Bois aux Daims</td>\n",
       "      <td>août 2017</td>\n",
       "      <td>[bon, sejours, bonnes, activites, lieu, nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>14 août 2017</td>\n",
       "      <td>3</td>\n",
       "      <td>Très joli parc au milieu d'une forêt. Beaucoup...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Mitigé...</td>\n",
       "      <td>août 2017</td>\n",
       "      <td>[joli, milieu, d une, foret, beaucoup, d activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>14 août 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Je me permet de noter seulement les restaurant...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Ne pas aller aux restaurants</td>\n",
       "      <td>août 2017</td>\n",
       "      <td>[permet, noter, seulement, restaurants, l atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>14 août 2017</td>\n",
       "      <td>2</td>\n",
       "      <td>Nous venons de passer 10 jours au domaine, 1 w...</td>\n",
       "      <td>fr</td>\n",
       "      <td>séjour bien , mais pas parfait</td>\n",
       "      <td>août 2017</td>\n",
       "      <td>[venons, domaine, weeckend, 887 1, 345, situat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          hotel_name published_date  rating  \\\n",
       "1989  Center Parcs Le Bois aux Daims   14 août 2017       2   \n",
       "1990  Center Parcs Le Bois aux Daims   13 août 2017       4   \n",
       "1991  Center Parcs Le Bois aux Daims   14 août 2017       3   \n",
       "1992  Center Parcs Le Bois aux Daims   14 août 2017       1   \n",
       "1993  Center Parcs Le Bois aux Daims   14 août 2017       2   \n",
       "\n",
       "                                                 review review_language  \\\n",
       "1989  Cottage mal entretenu. Activités très onéreuse...              fr   \n",
       "1990  Très bon séjours... De très bonnes activités, ...              fr   \n",
       "1991  Très joli parc au milieu d'une forêt. Beaucoup...              fr   \n",
       "1992  Je me permet de noter seulement les restaurant...              fr   \n",
       "1993  Nous venons de passer 10 jours au domaine, 1 w...              fr   \n",
       "\n",
       "                                         title   trip_date  \\\n",
       "1989  de tous les center parcs, c'est le pire.   août 2017   \n",
       "1990                    séjours Bois aux Daims   août 2017   \n",
       "1991                                 Mitigé...   août 2017   \n",
       "1992             Ne pas aller aux restaurants    août 2017   \n",
       "1993            séjour bien , mais pas parfait   août 2017   \n",
       "\n",
       "                                                 tokens  \n",
       "1989  [mal, entretenu, activites, onereuses, mauvais...  \n",
       "1990  [bon, sejours, bonnes, activites, lieu, nature...  \n",
       "1991  [joli, milieu, d une, foret, beaucoup, d activ...  \n",
       "1992  [permet, noter, seulement, restaurants, l atte...  \n",
       "1993  [venons, domaine, weeckend, 887 1, 345, situat...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing transformations for preprocessing function\n",
    "caracters_to_remove = list(string.punctuation)\n",
    "transformation_car_dict = {initial:\" \" for initial in caracters_to_remove}\n",
    "\n",
    "with_accent = ['é', 'è', 'ê', 'à', 'ù', 'ç', 'ô', 'î']\n",
    "without_accent = ['e', 'e', 'e', 'a', 'u', 'c', 'o', 'i']\n",
    "transformation_accent_dict = {before:after for before, after in zip(with_accent, without_accent)}\n",
    "\n",
    "stopW = stopwords.words('french')\n",
    "stopW += ['les', 'a', 'tout'\"good\", \"great\", \"one\", \"nt\", \"u2013\", \"\", \"would\", \"get\", \"center\", \"parcs\",\"the\",\"le\"\n",
    "                   ,\" u00e0\",\"nous\",\"with\",\"what\",\"for\",\"etait\",\"not\",\"the\",\"une\",\"de\",\"pour\",\"plus\",\"zip\",\"under\",\"but\",\"tr u00e0\",\"   \",\n",
    "                  'n\\'t','...','\\'ve','\\'re', 'ã\\x9dou', \"''\", 'avez','est','être','être.on','le','les','de','et','la','pour','très','à',\n",
    "                'en','a','des','pas','dans','du','au','avec','sont','que','avons','de','et','pour'\n",
    "                ,'nous','un','est','en','des','pas','dans','du','au','avec','-\\\\taqua','-\\\\ten','-\\\\tle','farfelues.moi','\\\\u2764','amp','dis','\\\\u00bc',\n",
    "               'tré','tres','-\\\\tpas','-\\\\tla','1m01','cafetière\\\\u2026','une','tout','mais','qui','il','sur',\"c'est\",'ne','plus','ce','se',\n",
    "               'aux','je','peu','vraiment','par','vous','fait','faire','était','car','ou','quot','si','sans','tous','aussi','fois','n','ans','entre','peut',\n",
    "               'semaine','faut','voir','3','eu','cela','ont','nos','4','même','nos','contre','donc','surtour','son','reste','séjour','site','rapport',\n",
    "                'surtout','quand','2','sommes','sinon','vu','jours','assez','autres','\\u00e7a','ménage',\"n'est\",'toujours','soit','c','l','malgré','comme',\n",
    "                'niveau','toutes','non','avions','toute','\\u00e7a','mon','moins','passer','devant','où',\"c est\",\"trop\",\" u00e7a\",\"parc\",\"daims\",\"cottage\",\n",
    "                   \"sejour\",\"etre\",\"mundo\",\"rien\",'aqua',\"bois\",\"cottages\",\"passe\",\"l aqua\",\"d activites\",\"n'est\",\"j'ai\",'avant','apres','ils','alors',\n",
    "                   'trouve','deja',\"qu il\",'dit']\n",
    "\n",
    "\n",
    "# Preprocessing function to apply to the content column\n",
    "def preprocessing(review):\n",
    "  \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(review)\n",
    "    \n",
    "    # Deleting words with  only one caracter\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "    \n",
    "    # stopwords + lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopW]   \n",
    "    \n",
    "    # Removing accents\n",
    "    tokens = [token.translate(str.maketrans(transformation_accent_dict)) for token in tokens]\n",
    "    \n",
    "    # Deleting specific caracters\n",
    "    tokens = [token.translate(str.maketrans(transformation_car_dict)) for token in tokens]\n",
    "        \n",
    "    return tokens\n",
    "  \n",
    "\n",
    "# Creating a new column swith tokenized reviews\n",
    "df1['tokens'] = df1['review'].apply(preprocessing)\n",
    "\n",
    "# Displaying part of the result\n",
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-WARsCeo-JL"
   },
   "source": [
    "# Discovering [Stemming](https://en.wikipedia.org/wiki/Stemming) and [Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "\n",
    "\n",
    "If you want to understand how the [Porter Algorithm](https://fr.wikipedia.org/wiki/Racinisation#Algorithme_de_Porter) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-2VUwd4kKL2"
   },
   "source": [
    "## Visualizing the effects of two different stemmers on basic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EHK8XvxD3v_G",
    "outputId": "83c80b3a-2efa-4860-b12e-4bb9119d8b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMc4hDBnkT8n"
   },
   "source": [
    "## Effects on a total sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zVE0VoS4Pp9"
   },
   "outputs": [],
   "source": [
    "def stemSentence(sentence, stemmer):\n",
    "    \n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = []\n",
    "    \n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FZsE-n21lxwN",
    "outputId": "b98b81ae-f78e-4fcb-f355-5869d5e4c1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ar very intellig and work very python and now they ar python their way to success . \n",
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "# And compare differences\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Qcbb5PYKlzCD",
    "outputId": "ec9de27e-88be-4900-e74d-038a1e6dfd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce matin je sui allé achet un galet à la boulangery pui je me sui régalé av de venir en cour . \n",
      "Ce matin je sui allé achet une galett à la boulangeri pui je me sui régalé avant de venir en cour . \n"
     ]
    }
   ],
   "source": [
    "# Look at what is happening on a french sentence\n",
    "sentence=\"Ce matin je suis allé acheter une galette à la boulangerie puis je me suis régalé avant de venir en cours.\"\n",
    "\n",
    "print(stemSentence(sentence, lancaster))\n",
    "print(stemSentence(sentence, porter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxILJkRnkeZ2"
   },
   "source": [
    "## A stemmer to use on different languages (for example french..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iMi9p7VH4Rbx",
    "outputId": "7a5d169f-762b-481c-931f-840bcba2895b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cet phras est à la fois amus et surpren '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frenchStemSentence(sentence):\n",
    "    frenchStemmer=SnowballStemmer(\"french\", ignore_stopwords=False)\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(frenchStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "frenchStemSentence(\"cette phrase est à la fois amusante et surprenante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELSBOPDTorCM"
   },
   "source": [
    "## Having a look at lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y3nNOH9FWpM8",
    "outputId": "d52954c5-07b4-4e07-b46e-857ee852db7e"
   },
   "outputs": [],
   "source": [
    "# Initiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create lematizing function\n",
    "def lemmatize(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# And display results\n",
    "lemmer = lemmatize(\"Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such an analysis can reveal feature that be not easily visible from the variation in the individual gene and can lead to a picture of expression that be more biologically transparent'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUN-NholwS3a"
   },
   "source": [
    "# Applying one of those modification to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VP7Tn1Uy4tj"
   },
   "source": [
    " **Preparing both functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14FnlWXwoum9"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return tokens  \n",
    "\n",
    "# Stemming\n",
    "frenchStemmer=SnowballStemmer(\"french\")\n",
    "def stem(tokens):\n",
    "    tokens = [frenchStemmer.stem(token) for token in tokens]\n",
    "    return tokens  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZHHZdbjzBlI"
   },
   "source": [
    "**Selecting which one to apply, given the language used in your reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZIv9DttzLml"
   },
   "outputs": [],
   "source": [
    "# Are your reviews in English ? (here it is unfortunately not the case)\n",
    "english = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9VkIu1_zO2Z"
   },
   "source": [
    "**And finally applying it to our dataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "3QRw4_nLySif",
    "outputId": "c2bd7108-eb38-4b0a-b388-92eeef98393e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_language</th>\n",
       "      <th>title</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>Lemmatisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Premier séjour à center parc pour nous et nous...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Petite semaine en famille</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[premier, adore, partis, petite, fille, parfai...</td>\n",
       "      <td>[premier, adore, partis, petite, fille, parfai...</td>\n",
       "      <td>[premi, ador, part, petit, fill, parf, premuim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>sejour très agreable comme d habitude a center...</td>\n",
       "      <td>fr</td>\n",
       "      <td>center parcs la promesse des daims</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[agreable, habitude, quel, plaisir, terrasse, ...</td>\n",
       "      <td>[agreable, habitude, quel, plaisir, terrasse, ...</td>\n",
       "      <td>[agreabl, habitud, quel, plais, terr, eau, mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Cottage très bien, propre et chaud dès l'arriv...</td>\n",
       "      <td>fr</td>\n",
       "      <td>A améliorer</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[bien, propre, chaud, des, l arrivee, agreable...</td>\n",
       "      <td>[bien, propre, chaud, de, l arrivee, agreable ...</td>\n",
       "      <td>[bien, propr, chaud, de, l arrive, agreable un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>1 février 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Séjour très reposant à noter tout de même troi...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Reposant</td>\n",
       "      <td>février 2019</td>\n",
       "      <td>[reposant, noter, trois, restaurants, testes, ...</td>\n",
       "      <td>[reposant, noter, trois, restaurant, testis, q...</td>\n",
       "      <td>[repos, not, trois, restaur, test, quatr, troi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Center Parcs Le Bois aux Daims</td>\n",
       "      <td>2 février 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>Arrivés pour trois nuits et deux jours en sema...</td>\n",
       "      <td>fr</td>\n",
       "      <td>tres belle decouverte</td>\n",
       "      <td>janvier 2019</td>\n",
       "      <td>[arrives, trois, nuits, deux, hors, saison, n ...</td>\n",
       "      <td>[arrive, trois, nuits, deux, hors, saison, n e...</td>\n",
       "      <td>[arriv, trois, nuit, deux, hor, saison, n etio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       hotel_name  published_date  rating  \\\n",
       "0  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "1  Center Parcs Le Bois aux Daims  1 février 2019       5   \n",
       "2  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "3  Center Parcs Le Bois aux Daims  1 février 2019       3   \n",
       "4  Center Parcs Le Bois aux Daims  2 février 2019       5   \n",
       "\n",
       "                                              review review_language  \\\n",
       "0  Premier séjour à center parc pour nous et nous...              fr   \n",
       "1  sejour très agreable comme d habitude a center...              fr   \n",
       "2  Cottage très bien, propre et chaud dès l'arriv...              fr   \n",
       "3  Séjour très reposant à noter tout de même troi...              fr   \n",
       "4  Arrivés pour trois nuits et deux jours en sema...              fr   \n",
       "\n",
       "                                title      trip_date  \\\n",
       "0           Petite semaine en famille   janvier 2019   \n",
       "1  center parcs la promesse des daims   janvier 2019   \n",
       "2                         A améliorer   janvier 2019   \n",
       "3                            Reposant   février 2019   \n",
       "4               tres belle decouverte   janvier 2019   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [premier, adore, partis, petite, fille, parfai...   \n",
       "1  [agreable, habitude, quel, plaisir, terrasse, ...   \n",
       "2  [bien, propre, chaud, des, l arrivee, agreable...   \n",
       "3  [reposant, noter, trois, restaurants, testes, ...   \n",
       "4  [arrives, trois, nuits, deux, hors, saison, n ...   \n",
       "\n",
       "                                            Stemming  \\\n",
       "0  [premier, adore, partis, petite, fille, parfai...   \n",
       "1  [agreable, habitude, quel, plaisir, terrasse, ...   \n",
       "2  [bien, propre, chaud, de, l arrivee, agreable ...   \n",
       "3  [reposant, noter, trois, restaurant, testis, q...   \n",
       "4  [arrive, trois, nuits, deux, hors, saison, n e...   \n",
       "\n",
       "                                       Lemmatisation  \n",
       "0  [premi, ador, part, petit, fill, parf, premuim...  \n",
       "1  [agreabl, habitud, quel, plais, terr, eau, mon...  \n",
       "2  [bien, propr, chaud, de, l arrive, agreable un...  \n",
       "3  [repos, not, trois, restaur, test, quatr, troi...  \n",
       "4  [arriv, trois, nuit, deux, hor, saison, n etio...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making appropriate modification\n",
    "df1['Stemming'] = df1['tokens'].apply(lemmatize)\n",
    "\n",
    "df1['Lemmatisation'] = df1['tokens'].apply(stem)\n",
    "\n",
    "# And displaying results\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('clean_data_gr4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['premier', 'sejour', 'center', 'parc', 'adore...\n",
       "1       ['sejour', 'tres', 'agreable', 'comme', 'habit...\n",
       "2       ['cottage', 'tres', 'bien', 'propre', 'chaud',...\n",
       "3       ['sejour', 'tres', 'reposant', 'noter', 'trois...\n",
       "4       ['arrives', 'trois', 'nuits', 'deux', 'jours',...\n",
       "5       ['center', 'parc', 'endroit', 'tres', 'relaxan...\n",
       "6       ['agreablement', 'surpris', 'super', 'sejour',...\n",
       "7       ['piscine', 'bondee', 'monde', 'pourtant', 'ho...\n",
       "8       ['personnel', 'service', 'client', 'hauteur', ...\n",
       "9       ['vrai', 'paradis', 'deconnecter', 'travail', ...\n",
       "10      ['jours', 'detente', 'famille', 'niveau', 'l a...\n",
       "11      ['tres', 'beau', 'cadre', 'harmonie', 'natured...\n",
       "12      ['sejour', 'tres', 'agreable', 'tres', 'bon', ...\n",
       "13      ['parc', 'calme', 'nature', 'partout', 'cottag...\n",
       "14      ['tres', 'bonnes', 'installations', 'seul', 'b...\n",
       "15      ['venus', 'famille', 'amie', 'enfants', 'faire...\n",
       "16      ['passe', 'tres', 'bon', 'sejour', ' environne...\n",
       "17      ['cottage', 'vip', 'personnes', 'juste', 'supe...\n",
       "18      ['toujours', 'aussi', 'bien', 'vais', 'toujour...\n",
       "19      ['premier', 'sejour', 'center', 'parcs', 'fami...\n",
       "20      ['cottage', 'tres', 'bien', 'agence', 'famille...\n",
       "21      ['bel', 'endroit', 'tres', 'calme', 'niveau', ...\n",
       "22      ['bon', 'sejour', 'passe', 'famille', 'cote', ...\n",
       "23      ['passe', 'super', 'semaine', 'famille', 'cott...\n",
       "24      ['tres', 'depaysant', 'surtout', 'hiver tres',...\n",
       "25      ['emerveillement', 'enfants', 'l endroit', 'id...\n",
       "26      ['sejour', 'magique', 'famille', 'alles', 'sem...\n",
       "27      ['c est', 'bon', 'pouvoir', 'baigner', 'eau', ...\n",
       "28      ['geste', 'commercial', 'apprecie', 'compte', ...\n",
       "29      ['recommande', 'center', 'parc', 'toutes', 'pe...\n",
       "                              ...                        \n",
       "1964    ['cottage', 'tres', 'agreable', 'parcs', 'tres...\n",
       "1965    ['concept', 'reposant', 'daims', 'rendent', 'v...\n",
       "1966    ['bonnes', 'infrastructures', 'neuves', 'quali...\n",
       "1967    ['troisieme', 'sejour', 'center', 'parcs', 'id...\n",
       "1968    ['troisieme', 'fois', 'venons', 'passer', 'que...\n",
       "1969    ['passer', 'journee', 'center', 'parcs', 'avis...\n",
       "1970    ['partis', 'semaine', 'cottage', 'premium', 's...\n",
       "1971    ['domaine', 'magnifique', 'dommage', 'services...\n",
       "1972    ['sejour', 'center', 'parcs', 'grande', 'premi...\n",
       "1973    ['jours', 'passes', 'tres', 'agreable', 'cotta...\n",
       "1974    ['voulu', 'tester', 'parc', 'car', 'moins', 'l...\n",
       "1975    ['miss', 'ans', 'eblouies', 'eclatees', '   ',...\n",
       "1976    ['parc', 'piscine', 'cottage', 'tres', 'agreab...\n",
       "1977    ['emplacement', 'menage', 'moutons', 'poussier...\n",
       "1978    ['passes', 'agreable', 'sejour', 'center', 'pa...\n",
       "1979    ['enfant', 'tombe', 'nez', 'montant', 'escalie...\n",
       "1980    ['parc', 'magnifique', 'daims', 'bien', 'prese...\n",
       "1981    ['tres', 'sympa', 'center', 'parcs', 'ressour ...\n",
       "1982    ['vacances', 'sympa', 'cadre', 'daims', 'beaux...\n",
       "1983    ['tres', 'de u00e7u', 'bois', 'daimsc est', '2...\n",
       "1984    ['menage', 'fait', 'trop', 'vite', 'donc', 'ma...\n",
       "1985    ['formule', 'agreable', 'vacances', 'actives',...\n",
       "1986    ['partis', 'entre', 'amis', 'deux', 'enfants',...\n",
       "1987    ['personnel', 'tres', 'agreable', 'centre', 'a...\n",
       "1988    ['passe', 'tres', 'bon', 'sejour', 'domaine', ...\n",
       "1989    ['cottage', 'mal', 'entretenu', 'activites', '...\n",
       "1990    ['tres', 'bon', 'sejours', '   ', 'tres', 'bon...\n",
       "1991    ['tres', 'joli', 'parc', 'milieu', 'd une', 'f...\n",
       "1992    ['permet', 'noter', 'seulement', 'restaurants'...\n",
       "1993    ['venons', 'passer', 'jours', 'domaine', 'weec...\n",
       "Name: tokens, Length: 1994, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essai2 = pd.read_csv('clean_data_group_4.csv')\n",
    "\n",
    "essai2.head()\n",
    "\n",
    "essai2['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6faNnlpH3L8-"
   },
   "source": [
    "# Final modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "wsQgbDCUpSGG",
    "outputId": "7b238486-e3c6-468c-98d5-d2c693e723b5"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-13eb23d8937a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Why not doing the same on title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Finally keeping only necessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-095b5c29c872>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Deleting words with  only one caracter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \"\"\"\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \"\"\"\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \"\"\"\n\u001b[1;32m   1353\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Why not doing the same on title \n",
    "df['title'] = df['title'].apply(preprocessing).apply(stem)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Finally keeping only necessary columns\n",
    "del(df['review'])\n",
    "del(df['tokens'])\n",
    "\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "X_HEC_Session_3_Notebook_2_stemming.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
