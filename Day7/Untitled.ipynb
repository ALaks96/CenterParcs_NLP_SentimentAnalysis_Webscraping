{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/laks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import sklearn\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/laks/Desktop/Polytechnique/Cours/Datacamp Capgemini (MAP540)/Day7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() # can help to obtain the good paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your path to word2vec google vectors\n",
    "path_to_google_vectors = '/Users/laks/Desktop/Polytechnique/Cours/Datacamp Capgemini (MAP540)/Day5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_number</th>\n",
       "      <th>review</th>\n",
       "      <th>service</th>\n",
       "      <th>service_sentiment</th>\n",
       "      <th>activities</th>\n",
       "      <th>activities_sentiment</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost_sentiment</th>\n",
       "      <th>family</th>\n",
       "      <th>family_sentiment</th>\n",
       "      <th>food</th>\n",
       "      <th>food_sentiment</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>infra_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>groupe_1</td>\n",
       "      <td>We are veterans to the Center Parcs holiday an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>groupe_1</td>\n",
       "      <td>Lodge was fine, nice setting, centre is well l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>groupe_1</td>\n",
       "      <td>After the madness of Christmas, it was a well ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_number                                             review  service  \\\n",
       "0     groupe_1  We are veterans to the Center Parcs holiday an...      1.0   \n",
       "1     groupe_1  Lodge was fine, nice setting, centre is well l...      1.0   \n",
       "2     groupe_1  After the madness of Christmas, it was a well ...      1.0   \n",
       "\n",
       "  service_sentiment  activities  activities_sentiment  cost  cost_sentiment  \\\n",
       "0              -1.0         1.0                   1.0   1.0            -1.0   \n",
       "1              -1.0         1.0                   1.0   1.0            -1.0   \n",
       "2               1.0         0.0                   NaN   0.0             NaN   \n",
       "\n",
       "   family  family_sentiment  food food_sentiment  infrastructure  \\\n",
       "0     0.0               NaN   1.0           -1.0             1.0   \n",
       "1     0.0               NaN   0.0            NaN             1.0   \n",
       "2     1.0               1.0   0.0            NaN             0.0   \n",
       "\n",
       "   infra_sentiment  \n",
       "0             -1.0  \n",
       "1              1.0  \n",
       "2              NaN  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your data are not in the current directory, don't forget : PATH + 'X_HEC_label.csv'\n",
    "labeled_data = pd.read_csv('X_HEC_label.csv', sep = \",\", index_col = False)\n",
    "labeled_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2551, 14)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detected_language</th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>inflected</th>\n",
       "      <th>published_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_language</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trip_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Center Parcs Sherwood Forest</td>\n",
       "      <td>[clean, down, hill, look, corner]</td>\n",
       "      <td>31 octobre 2011</td>\n",
       "      <td>4</td>\n",
       "      <td>cleaness downing down hill  don t look in corn...</td>\n",
       "      <td>119968750</td>\n",
       "      <td>fr</td>\n",
       "      <td>5EB20A7D530F745C9237B7E5D60B61AB</td>\n",
       "      <td>centre parcs sherwood</td>\n",
       "      <td>[cleaness, downing, hill, look, corners]</td>\n",
       "      <td>octobre 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Center Parcs Sherwood Forest</td>\n",
       "      <td>[centre, parcs, sit, numerous, occasion, quite...</td>\n",
       "      <td>February 14, 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>we ve been to centre parcs sites on numerous o...</td>\n",
       "      <td>652429761</td>\n",
       "      <td>en</td>\n",
       "      <td>E8B0404BB28844D394A367DD969BA417</td>\n",
       "      <td>Nice location, shame about the selfish guest c...</td>\n",
       "      <td>[centre, parcs, sites, numerous, occasions, qu...</td>\n",
       "      <td>February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>Center Parcs Sherwood Forest</td>\n",
       "      <td>[little, apprehensive, read, recent, review, n...</td>\n",
       "      <td>February 16, 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>i was a little apprehensive reading some of th...</td>\n",
       "      <td>652801542</td>\n",
       "      <td>en</td>\n",
       "      <td>1C083AD19649761DEEF510997AFD804D</td>\n",
       "      <td>Amazing stay!</td>\n",
       "      <td>[little, apprehensive, reading, recent, review...</td>\n",
       "      <td>February 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  detected_language                    hotel_name  \\\n",
       "0                en  Center Parcs Sherwood Forest   \n",
       "1                en  Center Parcs Sherwood Forest   \n",
       "2                en  Center Parcs Sherwood Forest   \n",
       "\n",
       "                                           inflected     published_date  \\\n",
       "0                  [clean, down, hill, look, corner]    31 octobre 2011   \n",
       "1  [centre, parcs, sit, numerous, occasion, quite...  February 14, 2019   \n",
       "2  [little, apprehensive, read, recent, review, n...  February 16, 2019   \n",
       "\n",
       "   rating                                             review  review_id  \\\n",
       "0       4  cleaness downing down hill  don t look in corn...  119968750   \n",
       "1       3  we ve been to centre parcs sites on numerous o...  652429761   \n",
       "2       5  i was a little apprehensive reading some of th...  652801542   \n",
       "\n",
       "  review_language                       reviewer_id  \\\n",
       "0              fr  5EB20A7D530F745C9237B7E5D60B61AB   \n",
       "1              en  E8B0404BB28844D394A367DD969BA417   \n",
       "2              en  1C083AD19649761DEEF510997AFD804D   \n",
       "\n",
       "                                               title  \\\n",
       "0                              centre parcs sherwood   \n",
       "1  Nice location, shame about the selfish guest c...   \n",
       "2                                      Amazing stay!   \n",
       "\n",
       "                                              tokens       trip_date  \n",
       "0           [cleaness, downing, hill, look, corners]    octobre 2011  \n",
       "1  [centre, parcs, sites, numerous, occasions, qu...   February 2019  \n",
       "2  [little, apprehensive, reading, recent, review...   February 2019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data = pd.read_json('english_reviews.json') #, sep = \",\", index_col = False)\n",
    "unlabeled_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaness', 'downing', 'hill', 'look', 'corners']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data['tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just cleaning out the reviews that were badly annotated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = labeled_data[labeled_data.infrastructure != -1]\n",
    "labeled_data = labeled_data[labeled_data.cost != -1]\n",
    "labeled_data = labeled_data[labeled_data.family != 9]\n",
    "labeled_data = labeled_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\",\n",
    "                     \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\",\n",
    "                     \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\",\n",
    "                     \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\n",
    "                     \"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\",\n",
    "                     \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\",\n",
    "                     \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\",\n",
    "                     \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "                     \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\",\n",
    "                     \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\",\n",
    "                     \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \n",
    "                     \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\",\n",
    "                     \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\n",
    "                     \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \n",
    "                     \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\",\n",
    "                     \"front\", \"full\", \"further\", \"get\", \"give\", \"good\", \"great\", \"woburn\", \"go\",\n",
    "                     \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \n",
    "                     \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "                     \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "                     \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\",\n",
    "                     \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\",\n",
    "                     \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\",\n",
    "                     \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\n",
    "                     \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \n",
    "                     \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \n",
    "                     \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\",\n",
    "                     \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \n",
    "                     \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\",\n",
    "                     \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\",\n",
    "                     \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "                     \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \n",
    "                     \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\",\n",
    "                     \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "                     \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \n",
    "                     \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\",\n",
    "                     \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "                     \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\",\n",
    "                     \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_replacement(input_string):\n",
    "    character_mapping = {\"\\\\u00e9\": \"é\",\n",
    "                        \"\\\\u2019\": \"'\",\n",
    "                        \"\\\\\": \"\",\n",
    "                        \"\\\\u00fb\": \"û\",\n",
    "                        \"u00e8\": \"è\",\n",
    "                        \"u00e0\": \"à\",\n",
    "                        \"u00f4\": \"ô\",\n",
    "                        \"u00ea\": \"ê\",\n",
    "                        \"u00ee\": \"i\",\n",
    "                        \"u00fb\": \"û\",\n",
    "                        \"u2018\": \"'\",\n",
    "                        \"u00e2\": \"a\",\n",
    "                        \"u00ab\": \"'\",\n",
    "                        \"u00bb\": \"'\",\n",
    "                        \"u00e7\": \"ç\",\n",
    "                        \"u00e2\": \"â\",\n",
    "                        \"u00f9\": \"ù\",\n",
    "                        \"u00a3\": \"£\",\n",
    "                        }\n",
    "\n",
    "\n",
    "    for character in character_mapping:\n",
    "        input_string = input_string.replace(character, character_mapping[character])\n",
    "\n",
    "    input_string = input_string.lower()\n",
    "\n",
    "    characters_to_remove = [\"@\", \"/\", \"#\", \".\", \",\", \"!\", \"?\", \"(\", \")\", \"-\", \"_\", \"’\", \"'\", \"\\\"\", \":\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"]\n",
    "    transformation_dict = {initial: \" \" for initial in characters_to_remove}\n",
    "    no_punctuation_reviews = input_string.translate(str.maketrans(transformation_dict))\n",
    "\n",
    "    return no_punctuation_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_string):\n",
    "    return word_tokenize(input_string)\n",
    "\n",
    "def remove_stop_words(input_tokens, english_stopwords = english_stopwords):\n",
    "    return [token for token in input_tokens if token not in english_stopwords]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(tokens, lemmatizer = lemmatizer):\n",
    "    tokens = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data['review'] = labeled_data['review'].apply(lambda x: character_replacement(x))\n",
    "labeled_data['tokens'] = labeled_data['review'].apply(lambda x: tokenize(x))\n",
    "labeled_data['tokens'] = labeled_data['tokens'].apply(lambda token_list: [meaningful_word for meaningful_word in token_list if len(meaningful_word) > 3])\n",
    "labeled_data['tokens'] = labeled_data['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COST LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split on manually labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = {'tokens' : list(labeled_data['tokens'])[:2000],\n",
    "                'labels' : list(labeled_data['cost'])[:2000]}\n",
    "training_set = pd.DataFrame(training_set)\n",
    "\n",
    "\n",
    "test_set = {'tokens' : list(labeled_data['tokens'])[2000:],\n",
    "            'labels' : list(labeled_data['cost'])[2000:]}\n",
    "test_set = pd.DataFrame(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-arranging training set to take into account unlabeled tokens and their missing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[veterans, center, parcs, holiday, visited, on...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lodge, fine, nice, setting, centre, laid, eas...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[madness, christmas, earned, enjoyable, break,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[visited, centre, parcs, times, past, years, t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[stayed, family, group, january, january, days...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  labels\n",
       "0  [veterans, center, parcs, holiday, visited, on...     1.0\n",
       "1  [lodge, fine, nice, setting, centre, laid, eas...     1.0\n",
       "2  [madness, christmas, earned, enjoyable, break,...     0.0\n",
       "3  [visited, centre, parcs, times, past, years, t...     0.0\n",
       "4  [stayed, family, group, january, january, days...     1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_supervised_data = {'tokens' : list(training_set['tokens']) + list(unlabeled_data['tokens']),\n",
    "                        'labels' : list(training_set['labels']) + [-1]*len(unlabeled_data)}\n",
    "\n",
    "# We use -1 to encode unlabeled samples\n",
    "\n",
    "semi_supervised_data = pd.DataFrame(semi_supervised_data)\n",
    "semi_supervised_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2543, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.shape\n",
    "semi_supervised_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_number</th>\n",
       "      <th>review</th>\n",
       "      <th>service</th>\n",
       "      <th>service_sentiment</th>\n",
       "      <th>activities</th>\n",
       "      <th>activities_sentiment</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost_sentiment</th>\n",
       "      <th>family</th>\n",
       "      <th>family_sentiment</th>\n",
       "      <th>food</th>\n",
       "      <th>food_sentiment</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>infra_sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>groupe_1</td>\n",
       "      <td>we are veterans to the center parcs holiday an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[veterans, center, parcs, holiday, visited, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_number                                             review  service  \\\n",
       "0     groupe_1  we are veterans to the center parcs holiday an...      1.0   \n",
       "\n",
       "  service_sentiment  activities  activities_sentiment  cost  cost_sentiment  \\\n",
       "0              -1.0         1.0                   1.0   1.0            -1.0   \n",
       "\n",
       "   family  family_sentiment  food food_sentiment  infrastructure  \\\n",
       "0     0.0               NaN   1.0           -1.0             1.0   \n",
       "\n",
       "   infra_sentiment                                             tokens  \n",
       "0             -1.0  [veterans, center, parcs, holiday, visited, on...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(path_to_google_vectors + 'GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_vector_getter(word, wv = w2v) :\n",
    "    # returns the vector of a word\n",
    "    try:\n",
    "        word_array = wv[word].reshape(1,-1)\n",
    "        return word_array\n",
    "    except :\n",
    "        # if word not in google word2vec vocabulary, return vector with low norm\n",
    "        return np.zeros((1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_embedding(text, wv = w2v) :\n",
    "    # returns naïve document embedding\n",
    "    embeddings = np.concatenate([my_vector_getter(token) for token in text])\n",
    "    centroid = np.mean(embeddings, axis=0).reshape(1,-1)\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embedding(semi_supervised_data['tokens'][0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(semi_supervised_data), 300))\n",
    "\n",
    "for i in range(len(semi_supervised_data)) :\n",
    "    X[i] = document_embedding(semi_supervised_data['tokens'][i])\n",
    "    #X_values = X.values\n",
    "X_train = X[:2000]\n",
    "Y_train = training_set['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_set), 300))\n",
    "\n",
    "for i in range(len(test_set)) :\n",
    "    X_test[i] = document_embedding(test_set['tokens'][i])\n",
    "    \n",
    "#X_test_pca = pca.transform(X_test)\n",
    "Y_test = test_set['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spreading_model = LabelSpreading()\n",
    "model_s = label_spreading_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_s.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using count vectorization\n",
      "\n",
      "\n",
      "Accuracy : 0.5561694290976059\n",
      "Precision : 0.5421455938697318\n",
      "Sensitivity : 0.9929824561403509\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Using count vectorization\")\n",
    "print(\"\\n\")\n",
    "acc_count = accuracy_score(Y_test,pred)\n",
    "prec_count = precision_score(Y_test, pred)\n",
    "sens_count = recall_score(Y_test,pred)\n",
    "\n",
    "print(\"Accuracy :\", acc_count)\n",
    "print(\"Precision :\", prec_count)\n",
    "print(\"Sensitivity :\", sens_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,X_test), axis=0)\n",
    "Y_train = np.concatenate((Y_train,pred), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01672081,  0.07565586,  0.00013237, ..., -0.04164483,\n",
       "         0.03256852, -0.0333458 ],\n",
       "       [ 0.0295059 ,  0.04058451, -0.01116494, ..., -0.02598216,\n",
       "         0.02268141, -0.01136686],\n",
       "       [ 0.03492011,  0.01960308, -0.07121519, ..., -0.08272236,\n",
       "        -0.00484411, -0.01405074],\n",
       "       ...,\n",
       "       [-0.01097072,  0.02156581, -0.01297506, ..., -0.02637692,\n",
       "         0.06335892,  0.01691702],\n",
       "       [ 0.01863052,  0.10220649,  0.00900176, ..., -0.05984081,\n",
       "         0.10680413, -0.00205485],\n",
       "       [-0.00565216,  0.06269084,  0.00430571, ..., -0.03780914,\n",
       "         0.036409  ,  0.00056732]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
